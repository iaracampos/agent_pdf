{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnNsuuatSF2s"
   },
   "source": [
    "## Sistema Inteligente de Perguntas e Respostas para Documentos PDF\n",
    "\n",
    "Este notebook apresenta o desenvolvimento de um **sistema inteligente de Perguntas e Respostas aplicado a arquivos com formato PDF**.\n",
    "\n",
    "A solução foi inspirada e baseada no artigo “How Extract Data from PDF Using LangChain and Mistral” de Jose Chipana, que demonstra o uso de LangChain, embeddings e modelos de linguagem para extrair e consultar informações de PDFs.\n",
    "\n",
    "Foram utilizados os pdfs da disciplina como dados de entrada para o sistema\n",
    "\n",
    "Referencias:\n",
    "\n",
    "[How extract data from PDF using LangChain and Mistral](https://medium.com/@chipanajose/how-extract-data-from-pdf-using-langchain-and-mistral-74b252fd88a0)\n",
    "\n",
    "[all-mpnet-base-v2 - modelo de embeddings](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)\n",
    "\n",
    "[guide to evaluate RAG](https://www.evidentlyai.com/llm-guide/rag-evaluation)\n",
    "\n",
    "\n",
    "[ library for evaluating and testing Large Language Model (LLM) applications](https://docs.ragas.io/en/stable/getstarted/#step-4-run-your-evaluation)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FKM3LLctXFLk",
    "outputId": "b632ae8a-5a47-4880-c104-c3118afd9dcd"
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain-core langchain-community langchain-mistralai langchain-text-splitters faiss-cpu ragas pypdf --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FT4JDN4hY2Xi"
   },
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from google.colab import files\n",
    "from langchain_core.runnables import RunnableMap\n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "nRDurnOgYtuv",
    "outputId": "0bcbd343-fc7d-4deb-e89f-504cdf816b2c"
   },
   "outputs": [],
   "source": [
    "#fazendo upload\n",
    "uploaded = files.upload()\n",
    "#pega nomes\n",
    "pdf_paths = list(uploaded.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vPR2TTLewrad",
    "outputId": "32bc5af1-8213-4f3f-fc40-4cfac3788c0f"
   },
   "outputs": [],
   "source": [
    "#lendo os pdfs e transformando em list[document], cada document tem page_content e metada\n",
    "documents = []\n",
    "\n",
    "for path in pdf_paths:\n",
    "    loader = PyPDFLoader(path)\n",
    "    docs = loader.load()\n",
    "    documents.extend(docs)\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvK2BeLVSwfO"
   },
   "source": [
    "**Divisão do Texto em Chunks**\n",
    "\n",
    "Antes de alimentar o sistema RAG, o texto original é dividido em partes menores para facilitar a indexação e melhorar a recuperação de informações.\n",
    "\n",
    "Utilizamos o RecursiveCharacterTextSplitter com os seguintes parâmetros:\n",
    "\n",
    "- **chunk_size = 1000** caracteres  \n",
    "- **chunk_overlap = 150** caracteres de sobreposição\n",
    "\n",
    "Essas configurações ajudam a manter a continuidade sem perder contexto entre os trechos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RfM9oWGZw8hY",
    "outputId": "b2d8f7e8-3036-4fed-9160-cb6739d91de9"
   },
   "outputs": [],
   "source": [
    "#dividindo o texto em chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150\n",
    ")\n",
    "\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8eSNez9S_dG"
   },
   "source": [
    "**Criação dos Embeddings**\n",
    "\n",
    " Cada chunk de texto é transformado em uma representação vetorial, permitindo que o sistema realize buscas semânticas eficientes.\n",
    "\n",
    "\n",
    "Foi utilizado **sentence-transformers/all-mpnet-base-v2**\n",
    "\n",
    "\n",
    "Esse modelo é amplamente usado em tarefas de similaridade semântica por apresentar alto desempenho em benchmarks como o STS (Semantic Textual Similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532,
     "referenced_widgets": [
      "b10d43713ef3461c86e2279208bc0a84",
      "eaa050eb0cf34b75ab0206231451d6be",
      "225f3263156e4913804f569dde58ef2c",
      "14ac49b888f44fc78a7fdb0a475386a6",
      "1e62fedcd1bc420ba933776f6fd4f3d5",
      "61e31b7a06814a38a6d593890aed9aaa",
      "9148b1da521f4e32980b25f5091cdd2c",
      "a021459f6eb647b680d8f80ff61ca80c",
      "43f7de4cfc6644fda82a3f1b063d0419",
      "ea3b0a5bc914485883f6307c219b8f1d",
      "9d43095afa164f13ae44e1a41fcd2c9d",
      "ff58316ae8d8427cb99286093aaa6e16",
      "bbc68ad1dd124b05b52d4af2c8f47f2f",
      "e6300838c1964e42875824d22634eb2c",
      "05c82be35cd84359805beaa053617cf1",
      "743911553c6e44c3ba000caef2d04f0c",
      "64a647e2740f40228513a84ddce30d63",
      "9fbaa6714af64176bee50abc896526aa",
      "b2e4cbb85e1341d7985f0f79340df75a",
      "a9c5b2ff5f3947ac9e22604bb807a341",
      "4a5759a11d814626a643317a4f66201f",
      "ba031d9994644f2aba2700720fca973d",
      "ac7438a6f0884fc59845012712b4d5ad",
      "595b5ba665a949269c12d9e65d76e468",
      "a6400dda328c4a73bcb63fd5a4d876da",
      "6e521a7c0cad44d1ae01033cb5cd6312",
      "a5115c9ab0984a4f8982e6d7434fe050",
      "f838125f6f3a40889f9df6c675757e47",
      "fc0e4e0ed3724c8989f3c3aab48e153a",
      "0efe944ee41546919cd66b4378d865b6",
      "1de7101254074283b0c66036daa354c9",
      "e9d994a9c23046de9b6ab422d8b5225c",
      "f1c2d0b73a8341e3bfb15a15f3421f41",
      "69c47a7a295e47b886deba63f9d9e373",
      "8f3f39fd3ed74bc58020fca787823144",
      "88b256a3bc8349bfab8469a1a34ec429",
      "dc19ddffd2074a66ad2352bb15031811",
      "058482eb7f26427398c919152f112f8c",
      "e78dfe4e76854c3faa2abe8311bf47a1",
      "3025d3905f2e480da1ec361bcca7e90f",
      "c0d57bf4a4764ba88fede012adca9ae3",
      "acbad3c4b07b49dfb6f84020a562cbd9",
      "789a11feaefa40c6999a41460dd638e6",
      "d91a0529ee2b46c98b7c9317ab88e646",
      "7784748c1059451dae398d486ac5b5d6",
      "46dc8815dd584c299a0a2866567ff8dc",
      "167056314f0e469787f063910d65815a",
      "04e6e423551f41458d7a1b0114bbbc1a",
      "60a88f971bfc4e3e92c430ee0bfa973f",
      "de255030303541a4bc1e7827159dafce",
      "e9ad154c22e241f981790a2e6a9d06d4",
      "ac47ef2f68584580ada791a6faabc59c",
      "e1996dcf322f41a88195a0f071a61297",
      "478b39871d824da7aba965a9c0805e92",
      "271b508368d040eb8e6a6035998061e9",
      "e319e707fd5948c7bc6c0524d5c5e3d5",
      "a510b27b0be046debbb72fda4ced534e",
      "d57606e4b9bd4ff5b5c8de5c2821709f",
      "c85fe552b1ad4af19343f76247ca68e7",
      "b76bd283ad2148979b636d880a70fa41",
      "e289bc34d748458b998a9ec2a2336448",
      "22fa2b192d03485994aaa4def38aae5e",
      "a91801d721644241a9127cf039ba351c",
      "c333ca79e9c540ddb78caffcdac560d2",
      "4a574bd989104435854c170b714755f2",
      "dab2894a92fd4941b6989352d8c55825",
      "950093aa1b124774a740944ab73b7468",
      "69bd1264634a4168a588f03d3ac582cf",
      "9f7088b348734f0eaf8b349e76fdd307",
      "7480e83d39b3408b8df868cca40a618e",
      "5242879272bd43d0826dedaaacef5305",
      "ca727262b51d4d679b62f2885fe4615c",
      "95e7dd5a7aa24ca0abdb22186f785c0d",
      "3939e1f2c54b448eb25e5398a6914359",
      "dd78104dd0c54ac6b14bbb3a2cfd2b15",
      "432cb7ea04414339acb6c0f680e96033",
      "6fb3d1e79a3f4b84b034add71e1f5bdd",
      "20abec4364ca42d1bba5be8dcc63c6ea",
      "6a52ff6c57fe4761908fa3a6aa7ff7e8",
      "c123e2aa70314a9598de5f71ddf0e79b",
      "632e39be61174f2b82a334c6a2af75a3",
      "09ee9cd9ec244b02b9075a75fdd2962c",
      "74278a95bc814f4392b0cf924bb575ff",
      "f2b81146afd444a19860d7ab81bc0620",
      "5bd55edc12c34e7ba09e284b0b589554",
      "1440cb012dce4686a8c3aa206eedbd21",
      "d03e77ac1fb9403dae911e54218b1117",
      "329801a51fa34ea7b2426072ddae1505",
      "2b86514d121b4c2084852499ca43c116",
      "8936eed057944015847e5abee13cc2e9",
      "9b114f0241f64368858905d8b4a7bee0",
      "f2faa97ebc0d42ce9eb3ec3f235502f1",
      "d4424a45d8504895811788a12112cc41",
      "5add508d763b472a8d440bd87f0d3d6d",
      "2293104e1a8f452b9eeb628d1ace7231",
      "e64b4c33cec046cbbaed9c80c6c2e298",
      "8d2e5679b80e42fdaf8ad78c53e8326b",
      "dd6613be34874b329fd420e8d7d4c85d",
      "e162943aa3264a61ab01bf9ef91b3724",
      "164e709ccb7b440399d466fedfeff968",
      "f7a3821bd1574c5a8e846ebc2ddb06a5",
      "31e3d96016d94c1cab67d759fac8d730",
      "a2cce3e1bada46f89ceeae53663cac06",
      "4bfaaf14ecae429e82100d3c1478342a",
      "f76e736632d447c58114e3e5abe24a91",
      "98cd4657db7449eb9b4de50487f1da7f",
      "4dc3a72189dd4041aea848508a089a6c",
      "658813fc7caf4f7294ba5fe35ae6b249",
      "a240f3b5fcc145b6ba196b07c77e933f",
      "4ad89ee116c14282bb8c5cf7c2a395f5",
      "853ed465f7f3483a9ddb4de4f30e9581",
      "edc46228a37c4d6d83d0225506d0499f",
      "410cb94ab0754cc8ad7933d589cd0fbb",
      "756d803f228d44cca2aa697571ef3e38",
      "a7ca012c8cae4a9686b18397ad37a6ca",
      "d59825bfff3c4489a08ba94cd964b38f",
      "d2e9600771d743a88272a2d194550d28",
      "b9b919ad1ea441be8a2e73b51dae0d3d",
      "ddd887aab43045f7bc15dbc329910d09",
      "b9dbbf77da11425c86ea817ff46fb8e8",
      "3d2b87b7ea1b4e1eb7d7f42917f342bb"
     ]
    },
    "id": "7gs0BmoWx8OR",
    "outputId": "efb3d247-466c-42f8-867a-88cd1774bd10"
   },
   "outputs": [],
   "source": [
    "#criando os embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKrJzH9xTWYd"
   },
   "source": [
    "**Armazenamento dos Embeddings com FAISS**\n",
    "\n",
    "Para permitir buscas rápidas e eficientes pelos vetores gerados, utilizamos o **FAISS**, uma biblioteca otimizada para operações de similaridade em grandes coleções de embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9lb-LsvyOrV"
   },
   "outputs": [],
   "source": [
    "#db eficiente para armazenar embeddings\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vi8HoCLZTmcZ"
   },
   "source": [
    "**Instanciando  Mistral**\n",
    "\n",
    "O modelo utilizado no pipeline RAG é o **Mistral Small**, acessado via API.\n",
    "\n",
    "A instanciação do LLM é feita através do nome do modelo e a da chave de acesso.\n",
    "\n",
    " - A chave foi deixada explícita apenas para fins de teste, já que expira em um dia.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwM-sBGey-Fc"
   },
   "outputs": [],
   "source": [
    "#instancia modelo, deixei a chave porque ela vence em um dia\n",
    "llm = ChatMistralAI( model=\"mistral-small-latest\", api_key=\"key\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A8JtK-Bn3KkI",
    "outputId": "a37be334-b813-4805-ca57-0aa1badb9725"
   },
   "outputs": [],
   "source": [
    "#testando mistral\n",
    "resp = llm.invoke(\"O que é uma rede neural?\")\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EU_QHUQu_Gq5"
   },
   "outputs": [],
   "source": [
    "#criando prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um assistente especializado extração e busca de respostas em pdfs/ .\"),\n",
    "    (\"human\", \"Use os trechos abaixo para responder a pergunta:\\n\\n{context}\\n\\nPergunta: {question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0iIaOTqFEoC"
   },
   "outputs": [],
   "source": [
    "#formantando doc\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEf67wueRuhJ"
   },
   "source": [
    "O pipeline abaixo utiliza **RunnableMap** para estruturar o fluxo do RAG.\n",
    "Ele realiza três etapas principais:\n",
    "\n",
    "1. **Recuperação de contexto** a partir da pergunta.\n",
    "2. **Geração da resposta** usando o LLM com o contexto recuperado.\n",
    "3. **Formatação da saída final** contendo pergunta, resposta e contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkJ-NF9oFGXM"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#pipeline que implementa RAG\n",
    "rag_chain = (\n",
    "    RunnableMap({\n",
    "        \"context\": itemgetter(\"question\") | retriever | format_docs,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    })\n",
    "    | {\n",
    "        \"answer_output\": prompt | llm,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"context\": itemgetter(\"context\"),\n",
    "    }\n",
    "    | RunnableMap({\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"answer\": lambda x: x[\"answer_output\"].content,\n",
    "        \"context\": itemgetter(\"context\"),\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGAWEq38SC-m"
   },
   "source": [
    "**Execução do Pipeline RAG**\n",
    "\n",
    "Nesta etapa, avaliamos o comportamento do pipeline RAG utilizando o método invoke.A partir de cada questão enviada, o pipeline retorna:\n",
    "\n",
    "- **Pergunta enviada ao modelo**\n",
    "- **Resposta gerada pelo LLM**\n",
    "- **Contextos recuperados pelo retriever**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Oix1rl8K8Qv",
    "outputId": "833088de-a529-4d78-b82f-1b9ad0dfa56a"
   },
   "outputs": [],
   "source": [
    "\n",
    "resposta = rag_chain.invoke({\"question\": \"Do que falam os slides?\"})\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmYCvW3-LC4n",
    "outputId": "9a2b59fd-7701-47c8-a5bc-8b2288f6f5da"
   },
   "outputs": [],
   "source": [
    "resposta = rag_chain.invoke({\"question\": \"O que é convolução sobre volumes ?\"})\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MnrRnzcULxwe",
    "outputId": "0fb2b95a-e04b-4f4d-d9c0-dbadbe4b7864"
   },
   "outputs": [],
   "source": [
    "resposta = rag_chain.invoke({\"question\": \"Qual slide fala sobre DeepFace ?\"})\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uSvQbQTaM_tR",
    "outputId": "1c047f2d-f7b3-4b4c-d295-6898febceafe"
   },
   "outputs": [],
   "source": [
    "resposta = rag_chain.invoke({\"question\": \"O que fala sobre Extração/Geração de Respostas de um Texto?\"})\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wl68tdKFN2mr",
    "outputId": "d1095e23-328a-47fd-ea2d-65c022dbadd8"
   },
   "outputs": [],
   "source": [
    "resposta = rag_chain.invoke({\"question\": \"Qual é a efiencia do Transformer ?\"})\n",
    "\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lCnn5kJyYW-h",
    "outputId": "a0fd111b-ebdf-485e-c467-8b5ffd00b7f0"
   },
   "outputs": [],
   "source": [
    "resposta = rag_chain.invoke({\"question\": \"Como funciona enconder e decoder?\"})\n",
    "\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ahnH3q0lYwxY",
    "outputId": "d5f26c39-0557-451b-aba9-1f885ba38c6e"
   },
   "outputs": [],
   "source": [
    "resposta = rag_chain.invoke({\"question\": \"Qual a arquitetura das RNNS?\"})\n",
    "\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ujcvXm1IZME9",
    "outputId": "8639803c-cbdc-43bc-e3f9-9777cb821983"
   },
   "outputs": [],
   "source": [
    "resposta = rag_chain.invoke({\"question\": \"Em qual pagina fala sobre a função de ativação Softmax?\"})\n",
    "\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eU4jpxMwZayr",
    "outputId": "9316fdf6-ff74-4dd1-c2c9-1e5f1c954df1"
   },
   "outputs": [],
   "source": [
    "resposta = rag_chain.invoke({\"question\": \"O que é Rede Neural Convolucional?\"})\n",
    "\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kd0d5SG68UQr",
    "outputId": "90af3114-2251-49c3-a87d-2fd49f8afb44"
   },
   "outputs": [],
   "source": [
    "resposta = rag_chain.invoke({\"question\": \"Como é arquitetura do GPT?\"})\n",
    "\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKFeP3jbUGE7"
   },
   "source": [
    "#### Avaliação do RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXotY4nJUJ36"
   },
   "outputs": [],
   "source": [
    "from ragas.metrics import (answer_correctness,faithfulness, context_recall,answer_relevancy)\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3c44a37"
   },
   "outputs": [],
   "source": [
    "#pega questions, respostas do rag, groud_truth, contex_retrieved\n",
    "\n",
    "questions = []\n",
    "rag_answers = []\n",
    "ground_truths = []\n",
    "retrieved_contexts_list = []\n",
    "\n",
    "for gt_entry in ground_truth_dataset:\n",
    "    question = gt_entry[\"question\"]\n",
    "    ground_truth = gt_entry[\"answer\"]\n",
    "\n",
    "    resposta = rag_chain.invoke({\"question\": question})\n",
    "    questions.append(question)\n",
    "    rag_answers.append(resposta[\"answer\"])\n",
    "    ground_truths.append(ground_truth)\n",
    "    contexts_list = resposta[\"context\"].split('\\n\\n')\n",
    "    retrieved_contexts_list.append(contexts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45f7d2a0"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "eval_data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": rag_answers,\n",
    "    \"ground_truth\": ground_truths,\n",
    "    \"contexts\": retrieved_contexts_list\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(eval_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "c267aaebdc9e45ea9faa5b4b7da22f3d",
      "11e42e72f6a54603b636105590bb33d2",
      "857c3462dd2a43feb45b77f84822266b",
      "2fc45e212a004e81a19c4612bff5f5e5",
      "ec45784aad8b4e859165a093ccd8b337",
      "efd67c0bf15143669ee42edfe87b260b",
      "3a47b49792544186afe19f776eb0d266",
      "953429502bcc477596e25d05c10e7259",
      "5efe6133795f4911aaaafa0581fe2675",
      "a7a36e7e64ea479a9b865e2291af2cd8",
      "22cfb1405bd84075ab4c80781698cf02"
     ]
    },
    "id": "4cae5ecf",
    "outputId": "1ce88b20-3045-47ad-8e7f-1f52f7ab9dc6"
   },
   "outputs": [],
   "source": [
    "result = evaluate(\n",
    "    dataset,\n",
    "    metrics=[\n",
    "        answer_correctness,   # equivalente a acurácia\n",
    "        faithfulness,         # equivalente a precisão\n",
    "        context_recall,       # equivalente a recall\n",
    "        answer_relevancy      # equivalente a F1\n",
    "    ],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlQKVsnKZ_vs"
   },
   "source": [
    "### Análise das Métricas\n",
    "\n",
    "As metricas obtidas na avaliação do sistema RAG utilizando o modelo **Mistral** indicam um desempenho moderado.\n",
    "\n",
    "**Answer Correctness:** 0.6494\n",
    "\n",
    "O modelo gera respostas semanticamente coerentes com o ground truth em 68% dos casos. Esse valor é esperado para um modelo de porte médio e um RAG simples.\n",
    "\n",
    "**Faithfulness:**  0.7250\n",
    "\n",
    "As respostas estão razoavelmente alinhadas aos trechos recuperados.O  modelo não alucina em excesso, mas ainda pode melhorar.\n",
    "\n",
    "**Context Recall:**  0.50\n",
    "Apenas metade das informações relevantes foram recuperadas pelo retriever. Mostra que o retriever é um ponto no RAG que precisa de melhoria.\n",
    "\n",
    "\n",
    "**Answer Relevancy:** 0.5995\n",
    "As respostas são moderamente relevantes para a pergunta.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyivtLQUGc-5"
   },
   "source": [
    "### Comparação experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ax7roHoZFdED"
   },
   "source": [
    "###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195,
     "referenced_widgets": [
      "fe16b60eccc14006b13896429ded541e",
      "560c8d7acdd34ee0ac7d4c420c34ac2f",
      "315774c6dde941c7967757df1877ab0d",
      "25a1827179b44dff8cb7f85c46fafd5f",
      "803632bbd0b6481b985d6736d7effc37",
      "f3e81f4d6a9f490abc1bc5800f63068a",
      "552596870cda49f3a915ab0ae2737853",
      "bf796ed88c214592865b6e4b35911aa1",
      "994f60f3235940d7b22f1bc666bfc5b1",
      "11a4d09bf7f94150bde8fc848bfb3af9",
      "7a6e9100f50f4599ab6d684af3208b7a",
      "ff6cd91e383943d2a60734deea1bc247",
      "aedc62d029ec4c87b61e9e862bc1905c",
      "bb55620d69784a15b74f9271fc7b8874",
      "d5d4ba41185244bc9ef93fc965bb0b1c",
      "9dc21da61b234923837052e13a21a912",
      "52c9fd8427a54bcab200efd52fc15dee",
      "77926e92925f4173bbc3b47e49d13e30",
      "a5fab86cc0864ea5a7a110a3f73efb99",
      "da6f1702393a4735ba4a4746fa624f22",
      "d2d2958798884ef4ac9362d832dffcf3",
      "18814e93c217461da3f32f3651dc664c",
      "ebc33ccd1ab9403a89543fd2d7d08ff2",
      "fe5b3d243a0e4858815ba532c26ca285",
      "1ed55a973b34403bbe5bf11a39d045ab",
      "f94870fed9564eecae1926f406cb07f2",
      "1fa3103baa1249ddba495f89af2d46f6",
      "3e8823b731444a109f4a08b874e7d551",
      "b0389e6a0d6444998fafae262f5b50a2",
      "f7a940a469a745c1acf0ddaf6a1086c9",
      "2519a07c0af6447893b315c4153497e8",
      "14b27711a6f04a5f86de8f78dee78ecb",
      "d893cbfe6e444e969d3eb1b34edf816e",
      "4e15694090054c038688ddc9eda3ebe0",
      "f86cbb7d9b344155b021b20a7105a599",
      "e9efa2cdfb4f49198212d91d6727cea7",
      "05e8da8285ba4095bf08c8daa37ed425",
      "d432f72c59bd4f81a05dd21c7c7458f8",
      "8669515aea3d4db3a15412129f73fbc2",
      "a56d67df62c1408a920a75fd976d4835",
      "5dcde848d1434a848ff6c607e53bc757",
      "e76678cd85e44860827f1903dc7e9092",
      "a2d901358fee470893c9e2775633a456",
      "4fe90d232df844dd8fb2e5969c932fea",
      "6e3c0b554a5e4b11bac83a794a1ecae3",
      "a8339a74cf1c41eeb1827bdf2596c00c",
      "f93651bc61eb4e39808e0e3e3ab69373",
      "aa66cd601e73415a8e6fc911aadb646c",
      "3351fb8bf30e4c09976dbc2ec09e3479",
      "1e9f684bd1c4401397e29219d703335a",
      "4f3b1c9d704b4143a8ebe36b88151992",
      "b9867775c0cd4fd890fef37ce100c175",
      "a0128308a3f64c8dab701fc05629bbad",
      "f173a97964e54bbdb7af92661d889dd5",
      "51ebec5ef6764b7bac378f9e8de44bd0"
     ]
    },
    "id": "Gjsr5vKoGkKS",
    "outputId": "1708f868-931a-4e62-81c6-5d83a76a0fa8"
   },
   "outputs": [],
   "source": [
    "#troca embedding para experimento\n",
    "embeddings_base = HuggingFaceEmbeddings(\n",
    "    model_name=\"bert-base-uncased\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRCCT2HvHVoP"
   },
   "outputs": [],
   "source": [
    "#db eficiente para armazenar embeddings\n",
    "vector_store_experiment = FAISS.from_documents(chunks, embeddings_base)\n",
    "\n",
    "retriever_experiment = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duOCQDLoI3WU"
   },
   "outputs": [],
   "source": [
    "#pipeline que implementa RAG\n",
    "rag_chain = (\n",
    "    RunnableMap({\n",
    "        \"context\": itemgetter(\"question\") | retriever_experiment | format_docs,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    })\n",
    "    | {\n",
    "        \"answer_output\": prompt | llm,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"context\": itemgetter(\"context\"),\n",
    "    }\n",
    "    | RunnableMap({\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"answer\": lambda x: x[\"answer_output\"].content,\n",
    "        \"context\": itemgetter(\"context\"),\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJfV2-PFJB-s",
    "outputId": "152f52e6-8897-4793-b807-bee1d8de192b"
   },
   "outputs": [],
   "source": [
    "\n",
    "resposta = rag_chain.invoke({\"question\": \"Do que falam os slides?\"})\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ye9V22uTJit9",
    "outputId": "94101a77-fe16-436a-effc-a95092ffaeec"
   },
   "outputs": [],
   "source": [
    "resposta = rag_chain.invoke({\"question\": \"O que é convolução sobre volumes ?\"})\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zu69D0C9JkgO",
    "outputId": "723e74e8-4fad-40ba-8347-a91da186167b"
   },
   "outputs": [],
   "source": [
    "resposta = rag_chain.invoke({\"question\": \"Qual slide fala sobre DeepFace ?\"})\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ipVPD0YJoU_",
    "outputId": "7ee59a00-a9c4-45c7-d4f0-bc1aa5588462"
   },
   "outputs": [],
   "source": [
    "resposta = rag_chain.invoke({\"question\": \"Qual slide fala sobre DeepFace ?\"})\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WrnA5yGfJqbg",
    "outputId": "ec7c4c2c-c649-43b0-b2f7-1279cc11e198"
   },
   "outputs": [],
   "source": [
    "resposta = rag_chain.invoke({\"question\": \"O que fala sobre Extração/Geração de Respostas de um Texto?\"})\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-QbKRXwJs-m",
    "outputId": "0f4d3c5b-43ef-42f3-c14b-1270c67b0eb0"
   },
   "outputs": [],
   "source": [
    "\n",
    "resposta = rag_chain.invoke({\"question\": \"Qual é a efiencia do Transformer ?\"})\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5wivl3SpJweJ",
    "outputId": "a9ef4a3b-14c4-4edd-d0c8-6eb77e0a35bc"
   },
   "outputs": [],
   "source": [
    "resposta = rag_chain.invoke({\"question\": \"Como funciona enconder e decoder?\"})\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTdTZwn5Jz6U",
    "outputId": "12aaf7c7-092e-420f-84a9-c92376f31e77"
   },
   "outputs": [],
   "source": [
    "\n",
    "resposta = rag_chain.invoke({\"question\": \"Qual a arquitetura das RNNS?\"})\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VT6jgPF-J25e",
    "outputId": "0feea223-23ac-48bf-8f77-6f7cc44adb6a"
   },
   "outputs": [],
   "source": [
    "resposta = rag_chain.invoke({\"question\": \"Em qual pagina fala sobre a função de ativação Softmax?\"})\n",
    "\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "krKnC9EUJ5NP",
    "outputId": "264c64a8-9785-4f6a-9144-c9dd84971fec"
   },
   "outputs": [],
   "source": [
    "\n",
    "resposta = rag_chain.invoke({\"question\": \"O que é Rede Neural Convolucional?\"})\n",
    "\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ns5ATTX-J7SX",
    "outputId": "016e6092-e921-4557-e104-2a426142b0e1"
   },
   "outputs": [],
   "source": [
    "resposta = rag_chain.invoke({\"question\": \"Como é arquitetura do GPT?\"})\n",
    "\n",
    "\n",
    "print(\"Pergunta:\", resposta[\"question\"])\n",
    "print(\"Resposta:\", resposta[\"answer\"])\n",
    "print(\"Contextos:\", resposta[\"context\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZSzDol3J_NE"
   },
   "source": [
    "### Avaliando outra config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3ByV-oOKHXe"
   },
   "outputs": [],
   "source": [
    "#pega questions, respostas do rag, groud_truth, contex_retrieved\n",
    "\n",
    "questions = []\n",
    "rag_answers = []\n",
    "ground_truths = []\n",
    "retrieved_contexts_list = []\n",
    "\n",
    "for gt_entry in ground_truth_dataset:\n",
    "    question = gt_entry[\"question\"]\n",
    "    ground_truth = gt_entry[\"answer\"]\n",
    "\n",
    "    resposta = rag_chain.invoke({\"question\": question})\n",
    "    questions.append(question)\n",
    "    rag_answers.append(resposta[\"answer\"])\n",
    "    ground_truths.append(ground_truth)\n",
    "    contexts_list = resposta[\"context\"].split('\\n\\n')\n",
    "    retrieved_contexts_list.append(contexts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZ8NEJaCKSmV"
   },
   "outputs": [],
   "source": [
    "eval_data_experiment = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": rag_answers,\n",
    "    \"ground_truth\": ground_truths,\n",
    "    \"contexts\": retrieved_contexts_list\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(eval_data_experiment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "471c57d4951d4f259f6dfd3ea06ef062",
      "14f7271d2158434b878cf65b4c1dd5f6",
      "c817f04a128147eaa52d3570c114adeb",
      "78902f51819d430aa85c19e851dd2e28",
      "49d9d711150e425ab6535faf37f362d3",
      "67c6bdfd1cdc45e38f4674299d17bceb",
      "0d3bf90db561478bbd38cc1bf3e344ba",
      "ffdb1d8aa41d41f3aeeafd506934f8a1",
      "8c7ac9bdf46641ca95793d80ca477b92",
      "a9898ce8c4024e07b7e9bc5fe4089baf",
      "6a15e5514f044fa088664e2d70d4f555"
     ]
    },
    "id": "pxbF_4cCKXfG",
    "outputId": "e59205c0-ce7d-4974-afce-529d073ac5ea"
   },
   "outputs": [],
   "source": [
    "\n",
    "result = evaluate(\n",
    "    dataset,\n",
    "    metrics=[\n",
    "        answer_correctness,   # equivalente a acurácia\n",
    "        faithfulness,         # equivalente a precisão\n",
    "        context_recall,       # equivalente a recall\n",
    "        answer_relevancy      # equivalente a F1\n",
    "    ],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ag63PpZuK9WS"
   },
   "source": [
    "**Análise das Métricas**\n",
    "\n",
    "\n",
    "\n",
    "Foi feita uma execução do experimento RAG, com mudança de embeddings para o modelo de embeddings\n",
    "[bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As métricas obtidas na nova avaliação do sistema RAG utilizando o modelo Mistral mostram uma performance moderada, com algumas melhorias e outras quedas em relação à execução anterior.\n",
    "\n",
    "\n",
    "\n",
    "**Answer Correctness:** 0.6063\n",
    "\n",
    "O modelo produziu respostas semanticamente compatíveis com o ground truth em cerca de 61% dos casos.\n",
    "O desempenho caiu um pouco em relação ao experimento anterior, devido à mudança nos embeddings ou ao conjunto de chunks recuperados. Mesmo assim o resultado continua dentro do esperado para um pipeline RAG simples.\n",
    "\n",
    "**Faithfulness:** 0.8400\n",
    "\n",
    "O aumento de  Faithfulness sugere que o embedding atual ajudou o modelo a usar melhor o contexto que recebe. Foi a melhor métrica desse experimento.\n",
    "\n",
    "**Context Recall:** 0.50\n",
    "\n",
    "Assim como antes, apenas 50% das informações realmente relevantes foram recuperadas.\n",
    "\n",
    "\n",
    "**Answer Relevancy**: 0.5667\n",
    "\n",
    "O valor aumentou em relação ao experimento anterior (0.36 para 0.56), indicando que: as respostas agora estão mais conectadas às perguntas, o modelo está interpretando melhor o que o usuário deseja, e o novo embedding contribuiu para fornecer contexto mais alinhado as consultas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFK5LPOxPP-6"
   },
   "source": [
    "### Comparação entre os dois modelos de embeddings e justificativa dos resultados\n",
    "\n",
    "\n",
    "A comparação entre os dois experimentos mostra que a escolha do embedding impacta diretamente o desempenho do RAG.\n",
    "O **all-mpnet-base-v2** apresentou maior answer correctness, indicando respostas mais próximas ao ground truth, mas teve menor faithfulness e answer relevancy, sugerindo que o contexto recuperado nem sempre era adequado.\n",
    "\n",
    "No **bert-base-uncased**, houve melhora clara em faithfulness e answer relevancy, mostrando que o modelo passou a usar melhor o contexto e a alucinar menos, embora a correção total tenha caido um pouco.\n",
    "\n",
    "Em ambos os casos, o context recall permaneceu em 0.50, indicando que o principal limite do sistema está na recuperação de documentos, não no LLM ou no embedding."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
